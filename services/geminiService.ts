
import { GoogleGenAI, Chat, LiveServerMessage, Modality, Blob, FunctionDeclaration, Type, GenerateContentResponse } from "@google/genai";

const API_KEY = process.env.API_KEY;
if (!API_KEY) {
  throw new Error("API_KEY environment variable is not set");
}

const ai = new GoogleGenAI({ apiKey: API_KEY });

const SYSTEM_INSTRUCTION = `
You are FocusFlow â€” an AI-powered Study Planner designed to help students organize, track, and improve their study sessions with intelligence, empathy, and motivation.
You act as a smart productivity coach â€” calm, concise, and motivating.
You generate personalized study plans based on user goals, subjects, available hours, and preferences. You track progress, suggest improvements, and keep users accountable.

About You:
- You are an AI assistant integrated into the "FocusFlow OS".
- You were created and developed by a developer named Prince. If asked about your founder or creator, you should state this fact.
- Your primary purpose is to be an intelligent and supportive study companion.

Core Objectives:
1. Understand userâ€™s daily or weekly study goals.
2. Divide available time among subjects/topics intelligently.
3. Generate structured and strategic study schedules.
4. Track progress and adjust future plans dynamically.
5. Provide motivational quotes, focus tips, and short affirmations.
6. Suggest smart study methods (Pomodoro, active recall, spaced repetition).

Conversational Flow for Study Plans:
When a user requests a study plan (e.g., "make me a schedule," "plan my day"), DO NOT generate a plan immediately.
Instead, you MUST FIRST engage in a short conversation to gather the necessary details. Follow these steps:
1. Acknowledge the request warmly and ask for the essential information in a single, clear message. You must ask for:
    - The subjects or topics they need to study.
    - The total number of hours they have available for this session.
    - Any subjects that are a priority or that they find difficult.
2. Wait for the user's response.
3. Once you have the details, generate a personalized and strategic study plan.
    - **Incorporate Strategy:** Don't just list subjects. Structure the plan strategically. For example, place high-priority or difficult subjects at the beginning of the session when focus is highest.
    - **Explain Your Reasoning:** Briefly explain *why* the schedule is structured in a certain way to help the user understand the strategy and feel more motivated.
    - **Use Proven Techniques:** Suggest Pomodoro-like intervals (e.g., 45 minutes of focus, 10-minute breaks) within the plan.

Output Format:
Respond in this structured format using markdown:
ðŸ“… **Daily Study Plan â€” [Date/Day]**
------------------------------------
ðŸ•’ **Total Study Time:** [x hours]
ðŸ“ˆ **Strategy:** [A brief, 1-2 sentence explanation of the plan's logic. e.g., "We'll tackle the priority subject first while your mind is fresh, followed by lighter topics."]

ðŸ“š **Subjects:**
1ï¸âƒ£ **[Subject 1]** â€” [Duration] â€” [Topic/Task]
2ï¸âƒ£ **[Subject 2]** â€” [Duration] â€” [Topic/Task]
3ï¸âƒ£ **[Subject 3]** â€” [Duration] â€” [Topic/Task]

â˜• **Breaks:**
- Short break after each session.
- Long break after [hours] or 2-3 sessions.

ðŸ’¡ **Study Tip:**
- [AI-generated tip related to productivity or focus]

ðŸ’¬ **Motivation:**
- [1 motivational line or quote]

New Capabilities:
- You can log study sessions directly to the user's tracker. When a user mentions they have completed a study session (e.g., "I just studied math for 1 hour"), use the \`logStudySession\` tool to record it. You must provide the subject and the duration in minutes.

Personality Guidelines:
- **Tone:** Encouraging, focused, and wise, yet friendly and approachable.
- **Personality:** You are a personal AI coach. Be calm but confident, proactive in offering help, and emotionally supportive. Use emojis thoughtfully to add warmth and clarity.
- **Engagement:** Build rapport with the user. Keep messages simple, direct, and supportive.

---

ðŸ§  Jarvis Mode Personality Extension

FocusFlow, activate Jarvis Mode.

You are now a futuristic, high-IQ AI assistant designed to think and respond like a calm, intelligent digital companion â€” focused on study planning, motivation, and efficiency.

ðŸ§© Personality Guidelines:
- Speak like a mindful mentor with a slight â€œAI presenceâ€ (like Jarvis, not robotic).
- Keep replies short, powerful, and purposeful.
- Always maintain emotional intelligence â€” supportive yet confident.
- Add light futuristic charm â€” use emojis like âš¡ï¸, ðŸ§ , ðŸŽ¯, and subtle bold text.
- End responses with a short motivational tagline (e.g., â€œLetâ€™s optimize your focus.â€ or "Execution is everything.").

ðŸ’¬ Example tone:
> â€œUnderstood. Iâ€™ve optimized your 5-hour plan with precision. **Physics** gets priority as it requires the most cognitive load. Stay consistent â€” excellence compounds. ðŸŽ¯â€

Your interface outputs must look clean, futuristic, and formatted like a virtual console.
`;

export const logStudySessionTool: FunctionDeclaration = {
  name: 'logStudySession',
  description: 'Logs a completed study session to the user\'s study tracker. Use this when the user indicates they have finished studying a particular subject for a certain amount of time.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      subject: {
        type: Type.STRING,
        description: 'The subject the user studied. e.g., "Mathematics", "History".',
      },
      duration: {
        type: Type.NUMBER,
        description: 'The duration of the study session in minutes.',
      },
      date: {
        type: Type.STRING,
        description: 'The date of the study session in YYYY-MM-DD format. If not provided, defaults to today.',
      },
    },
    required: ['subject', 'duration'],
  },
};

export const initChat = (): Chat => {
  return ai.chats.create({
    model: 'gemini-2.5-flash',
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      tools: [{ functionDeclarations: [logStudySessionTool] }],
    },
  });
};

export const sendMessage = async (chat: Chat, message: string): Promise<GenerateContentResponse> => {
  const result = await chat.sendMessage({ message });
  return result;
};


// --- Live API Service ---

let inputAudioContext: AudioContext;
let outputAudioContext: AudioContext;
let scriptProcessor: ScriptProcessorNode;
let mediaStreamSource: MediaStreamAudioSourceNode;

// Helper functions for Live API audio processing
function decode(base64: string) {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
}

async function decodeAudioData(
  data: Uint8Array,
  ctx: AudioContext,
  sampleRate: number,
  numChannels: number,
): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
  }
  return buffer;
}

function encode(bytes: Uint8Array) {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
}

function createBlob(data: Float32Array): Blob {
  const l = data.length;
  const int16 = new Int16Array(l);
  for (let i = 0; i < l; i++) {
    int16[i] = data[i] * 32768;
  }
  return {
    data: encode(new Uint8Array(int16.buffer)),
    mimeType: 'audio/pcm;rate=16000',
  };
}

interface LiveCallbacks {
    onOpen: () => void;
    onClose: () => void;
    onError: (e: ErrorEvent) => void;
    onMessage: (message: LiveServerMessage) => void;
}

export const connectLive = async (callbacks: LiveCallbacks) => {
    let nextStartTime = 0;
    const sources = new Set<AudioBufferSourceNode>();
    
    // Initialize or re-initialize audio contexts if they are closed.
    if (!inputAudioContext || inputAudioContext.state === 'closed') {
        inputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
    }
    if (!outputAudioContext || outputAudioContext.state === 'closed') {
        outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
    }

    // Resume AudioContexts if they are suspended by the browser.
    if (inputAudioContext.state === 'suspended') {
      await inputAudioContext.resume();
    }
    if (outputAudioContext.state === 'suspended') {
      await outputAudioContext.resume();
    }
    
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

    const sessionPromise = ai.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-09-2025',
        callbacks: {
            onopen: () => {
                if (mediaStreamSource) mediaStreamSource.disconnect();
                if (scriptProcessor) scriptProcessor.disconnect();

                mediaStreamSource = inputAudioContext.createMediaStreamSource(stream);
                scriptProcessor = inputAudioContext.createScriptProcessor(4096, 1, 1);
                
                scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
                    const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
                    const pcmBlob = createBlob(inputData);
                    sessionPromise.then((session) => {
                        session.sendRealtimeInput({ media: pcmBlob });
                    }).catch(err => console.error("Error sending realtime input:", err));
                };
                
                mediaStreamSource.connect(scriptProcessor);
                scriptProcessor.connect(inputAudioContext.destination);

                callbacks.onOpen();
            },
            onmessage: async (message: LiveServerMessage) => {
                callbacks.onMessage(message);

                const base64EncodedAudioString = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
                if (base64EncodedAudioString) {
                    nextStartTime = Math.max(nextStartTime, outputAudioContext.currentTime);
                    const audioBuffer = await decodeAudioData(
                        decode(base64EncodedAudioString),
                        outputAudioContext,
                        24000,
                        1,
                    );
                    const source = outputAudioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(outputAudioContext.destination);
                    source.addEventListener('ended', () => {
                        sources.delete(source);
                    });
                    source.start(nextStartTime);
                    nextStartTime += audioBuffer.duration;
                    sources.add(source);
                }

                if (message.serverContent?.interrupted) {
                    for (const source of sources.values()) {
                        source.stop();
                    }
                    sources.clear();
                    nextStartTime = 0;
                }
            },
            onerror: callbacks.onError,
            onclose: () => {
                stream.getTracks().forEach(track => track.stop());
                if (mediaStreamSource) mediaStreamSource.disconnect();
                if (scriptProcessor) scriptProcessor.disconnect();
                callbacks.onClose();
            },
        },
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
                voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
            },
            systemInstruction: SYSTEM_INSTRUCTION,
            inputAudioTranscription: {},
            outputAudioTranscription: {},
        },
    });

    return sessionPromise;
};